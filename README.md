# Transformer Model - Classifica√ß√£o de Texto

Este projeto implementa um modelo Transformer do zero usando PyTorch para classifica√ß√£o de texto no dataset AG News.

## O que √© o Modelo Transformer?

O Transformer √© uma arquitetura de rede neural revolucion√°ria introduzida em 2017 no paper "Attention is All You Need". Diferente de RNNs e LSTMs, ele processa sequ√™ncias inteiras em paralelo usando mecanismos de aten√ß√£o.

### Principais Componentes

#### 1. **Multi-Head Attention**
- Permite que o modelo preste aten√ß√£o a diferentes partes da sequ√™ncia simultaneamente
- Divide a representa√ß√£o em m√∫ltiplas "cabe√ßas" (heads) que capturam diferentes aspectos do contexto
- Calcula scores de aten√ß√£o usando Query, Key e Value
- F√≥rmula: `Attention(Q, K, V) = softmax(QK^T / ‚àöd_k)V`

#### 2. **Positional Encoding**
- Adiciona informa√ß√£o sobre a posi√ß√£o dos tokens na sequ√™ncia
- Usa fun√ß√µes seno e cosseno para gerar encodings √∫nicos para cada posi√ß√£o
- Essencial porque o Transformer n√£o tem no√ß√£o inerente de ordem sequencial

#### 3. **Position-wise Feed-Forward Network**
- Duas camadas lineares com ativa√ß√£o GELU entre elas
- Aplicada independentemente a cada posi√ß√£o da sequ√™ncia
- Aumenta a capacidade de express√£o do modelo

#### 4. **Encoder Layer**
- Combina Multi-Head Attention + Feed-Forward Network
- Usa conex√µes residuais (skip connections) e Layer Normalization
- Estrutura: `X = LayerNorm(X + Attention(X))` ‚Üí `X = LayerNorm(X + FFN(X))`

#### 5. **Transformer Classifier**
- Stack de m√∫ltiplas camadas de Encoder
- Embedding de tokens e Positional Encoding na entrada
- Global Average Pooling para agregar a sequ√™ncia
- Cabe√ßa de classifica√ß√£o no topo

### Arquitetura do Projeto

```
Input Text
    ‚Üì
Token Embedding (vocab_size ‚Üí d_model)
    ‚Üì
Positional Encoding
    ‚Üì
Encoder Layer 1 (Multi-Head Attention + FFN)
    ‚Üì
Encoder Layer 2
    ‚Üì
    ...
    ‚Üì
Encoder Layer N
    ‚Üì
Global Average Pooling
    ‚Üì
Classification Head (Linear ‚Üí GELU ‚Üí Linear)
    ‚Üì
Output (num_classes)
```

## Configura√ß√£o do Ambiente Python

### Pr√©-requisitos
- Python 3.8 ou superior
- pip ou uv (gerenciador de pacotes)

### Criando o Ambiente Virtual

```bash
# Criar ambiente virtual
python -m venv venv

# Ativar o ambiente virtual
# macOS/Linux:
source venv/bin/activate

# Windows:
venv\Scripts\activate
```

### Instalando Depend√™ncias

```bash
# Op√ß√£o 1: Usando pip
pip install -r requirements.txt

# Op√ß√£o 2: Usando uv (mais r√°pido)
uv pip install -r requirements.txt
```

### Principais Depend√™ncias

- **torch**: Framework de deep learning
- **datasets**: Biblioteca da Hugging Face para carregar datasets
- **numpy**: Computa√ß√£o num√©rica
- **matplotlib** & **seaborn**: Visualiza√ß√£o de dados
- **scikit-learn**: M√©tricas de avalia√ß√£o

## üöÄ Executando o Projeto

1. Ative o ambiente virtual:
```bash
source venv/bin/activate  # macOS/Linux
```

2. Abra o notebook:
```bash
jupyter notebook transformer_model.ipynb
```

3. Execute as c√©lulas sequencialmente para:
   - Carregar o dataset AG News (4 categorias: World, Sports, Business, Sci/Tech)
   - Construir vocabul√°rio e tokenizar textos
   - Treinar o modelo Transformer
   - Avaliar performance e visualizar resultados

## Dataset

O projeto usa o **AG News**, um dataset de classifica√ß√£o de not√≠cias com:
- 120.000 amostras de treino
- 7.600 amostras de teste
- 4 categorias de not√≠cias

## Hiperpar√¢metros do Modelo

- `d_model`: 512 (dimens√£o do modelo)
- `num_heads`: 8 (n√∫mero de cabe√ßas de aten√ß√£o)
- `num_layers`: 6 (n√∫mero de camadas de encoder)
- `d_ff`: 2048 (dimens√£o da feed-forward network)
- `max_len`: 512 (comprimento m√°ximo da sequ√™ncia)
- `dropout`: 0.1

## Estrutura do C√≥digo

- `MultiHeadAttention`: Implementa o mecanismo de aten√ß√£o multi-cabe√ßa
- `PositionalEncoding`: Adiciona informa√ß√£o posicional aos embeddings
- `PositionwiseFeedForward`: Rede feed-forward position-wise
- `EncoderLayer`: Camada completa do encoder
- `TransformerClassifier`: Modelo completo para classifica√ß√£o

## Resultados Esperados

O modelo deve atingir boa acur√°cia na classifica√ß√£o de not√≠cias ap√≥s o treinamento, demonstrando a efic√°cia da arquitetura Transformer para tarefas de NLP.

---

**Nota**: Este projeto √© educacional e implementa o Transformer do zero para fins de aprendizado. Para aplica√ß√µes em produ√ß√£o, considere usar bibliotecas otimizadas como Hugging Face Transformers.
