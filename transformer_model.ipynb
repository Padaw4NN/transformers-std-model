{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tw5n0P2cOmV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import asyncio\n",
        "import warnings\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset # Library to load datasets easily\n",
        "from torch.cuda.amp import autocast, GradScaler # For mixed precision training\n",
        "from torch.utils.data import Dataset, DataLoader # For data handling\n",
        "\n",
        "from sklearn.metrics import confusion_matrix # For model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3H89k7rR8pz"
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter(\"ignore\") # Ignore warnings for cleaner output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vpIU7LCBMEN"
      },
      "source": [
        "### Preparção do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D_CqpWXBIIq",
        "outputId": "892de6bc-8030-4374-ca84-01200f0fcad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 120000\n",
            "Test samples: 7600\n",
            "\n",
            "Text 1 sample: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "Label 1 sample: Business\n",
            "\n",
            "Text 2 sample: Nets get Carter from Raptors INDIANAPOLIS -- All-Star Vince Carter was traded by the Toronto Raptors to the New Jersey Nets for Alonzo Mourning, Eric Williams, Aaron Williams, and a pair of first-round draft picks yesterday.\n",
            "Label 2 sample: Sports\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"ag_news\"); # Load the AG News dataset\n",
        "\n",
        "train_texts = dataset[\"train\"][\"text\"]\n",
        "train_labels = dataset[\"train\"][\"label\"]\n",
        "\n",
        "test_texts = dataset[\"test\"][\"text\"]\n",
        "test_labels = dataset[\"test\"][\"label\"]\n",
        "\n",
        "print(f\"Train samples: {len(train_labels)}\") # Print number of training samples\n",
        "print(f\"Test samples: {len(test_labels)}\") # Print number of testing samples\n",
        "\n",
        "print()\n",
        "\n",
        "label_map = { # Map numerical labels to human-readable categories\n",
        "    0: \"World\",\n",
        "    1: \"Sports\",\n",
        "    2: \"Business\",\n",
        "    3: \"Sci/Tech\",\n",
        "}\n",
        "\n",
        "print(f\"Text 1 sample: {train_texts[0]}\") # Display a sample training text\n",
        "print(f\"Label 1 sample: {label_map.get(train_labels[0])}\") # Display its corresponding label\n",
        "\n",
        "print()\n",
        "\n",
        "print(f\"Text 2 sample: {train_texts[-1]}\") # Display another sample training text\n",
        "print(f\"Label 2 sample: {label_map.get(train_labels[-1])}\") # Display its corresponding label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTRoRbBHxp58"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model:   int,\n",
        "                 num_heads: int,\n",
        "                 dropout:   float = 0.1) -> None:\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head\n",
        "\n",
        "        # Linear layers for Query, Key, Value, and output projection\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.d_k) # Scaling factor for attention scores\n",
        "\n",
        "    def split_heads(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        # Reshapes input for multi-head attention\n",
        "        batch_size, seq_len, d_model = X.size()\n",
        "        return X.view(\n",
        "            batch_size,\n",
        "            seq_len,\n",
        "            self.num_heads,\n",
        "            self.d_k\n",
        "        ).transpose(1, 2) # Transpose to get (batch_size, num_heads, seq_len, d_k)\n",
        "\n",
        "    def forward(self,\n",
        "                query: torch.Tensor,\n",
        "                key:   torch.Tensor,\n",
        "                value: torch.Tensor,\n",
        "                mask:  Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Project and split Query, Key, Value into multiple heads\n",
        "        Q = self.split_heads(self.W_q(query))\n",
        "        K = self.split_heads(self.W_k(key))\n",
        "        V = self.split_heads(self.W_v(value))\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e4) # Apply mask to attention scores\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1) # Apply softmax to get probabilities\n",
        "        attention_weights = self.dropout(attention_weights) # Apply dropout\n",
        "\n",
        "        context = torch.matmul(attention_weights, V) # Calculate weighted sum of values\n",
        "\n",
        "        # Concatenate heads and apply final linear projection\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch_size,\n",
        "            -1,\n",
        "            self.d_model\n",
        "        )\n",
        "\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11QCXH-d1ql4"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 d_ff:    int,\n",
        "                 dropout: float = 0.1) -> None:\n",
        "\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "\n",
        "        # Two linear layers with a non-linear activation (GELU) in between\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear2(self.dropout(F.gelu(self.linear1(X)))) # Forward pass: linear -> GELU -> dropout -> linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLLNPybs2buw"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 max_len: int = 5000,\n",
        "                 dropout: float = 0.1) -> None:\n",
        "\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model) # Initialize positional encoding matrix\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Positions for each token\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model) # Division term for sine/cosine arguments\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # Apply sine to even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # Apply cosine to odd indices\n",
        "\n",
        "        pe = pe.unsqueeze(0) # Add batch dimension\n",
        "        self.register_buffer(\"pe\", pe) # Register pe as a buffer, not a parameter\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        X = X + self.pe[:, :X.size(1), :] # Add positional embeddings to input embeddings\n",
        "        return self.dropout(X) # Apply dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PMHnopFMR8I"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model:   int,\n",
        "                 num_heads: int,\n",
        "                 d_ff:      int,\n",
        "                 dropout:   float = 0.1) -> None:\n",
        "\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout) # Multi-head self-attention module\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout) # Position-wise feed-forward network\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model) # Layer normalization after attention\n",
        "        self.norm2 = nn.LayerNorm(d_model) # Layer normalization after feed-forward\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout) # Dropout for attention output\n",
        "        self.dropout2 = nn.Dropout(dropout) # Dropout for feed-forward output\n",
        "\n",
        "    def forward(self,\n",
        "                X:    torch.Tensor,\n",
        "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        # Self-attention sub-layer with residual connection and layer normalization\n",
        "        attn_output = self.self_attention(X, X, X, mask)\n",
        "        X = self.norm1(X + self.dropout1(attn_output))\n",
        "\n",
        "        # Feed-forward sub-layer with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(X)\n",
        "        X = self.norm2(X + self.dropout2(ff_output))\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt06z6jyOsKS"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size:  int,\n",
        "                 num_classes: int,\n",
        "                 d_model:     int = 512,\n",
        "                 num_heads:   int = 8,\n",
        "                 num_layers:  int = 6,\n",
        "                 d_ff:    int = 2048,\n",
        "                 max_len: int = 512,\n",
        "                 pad_idx: int = 0,\n",
        "                 dropout: float = 0.1) -> None:\n",
        "\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.embedding = nn.Embedding( # Token embedding layer\n",
        "            vocab_size,\n",
        "            d_model,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "        self.positional_encoding = PositionalEncoding( # Positional encoding layer\n",
        "            d_model,\n",
        "            max_len,\n",
        "            dropout\n",
        "        )\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([ # Stack of Encoder Layers\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Sequential( # Classification head\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        self._init_weights() # Initialize model weights\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Custom weight initialization\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_padding_mask(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        # Create a boolean mask to ignore padding tokens\n",
        "        mask = (X != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        mask = self.create_padding_mask(X) # Generate padding mask\n",
        "\n",
        "        X = self.embedding(X) * math.sqrt(self.d_model) # Apply token embedding and scale\n",
        "        X = self.positional_encoding(X) # Add positional encoding\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            X = layer(X, mask) # Pass through each encoder layer\n",
        "\n",
        "        # Global average pooling on the output of the transformer encoder\n",
        "        mask_expanded = mask.squeeze(1).squeeze(1).unsqueeze(-1).float()\n",
        "        pooled = (X * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1)\n",
        "\n",
        "        logits = self.classifier(pooled) # Pass pooled output through the classifier\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kZQgxj7QzeV"
      },
      "outputs": [],
      "source": [
        "class  Vocabulary:\n",
        "    def __init__(self, max_vocab_size: int = 30000) -> None:\n",
        "\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "\n",
        "        # Special tokens for padding, unknown words, start-of-sentence, end-of-sentence\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n",
        "\n",
        "        self.word_counts = Counter() # To count word frequencies\n",
        "\n",
        "    def build_vocab(self, texts: list) -> None:\n",
        "        # Builds the vocabulary from a list of texts\n",
        "        for text in texts:\n",
        "            tokens = self.tokenize(text)\n",
        "            self.word_counts.update(tokens)\n",
        "\n",
        "        # Add most common words to vocabulary, excluding special tokens\n",
        "        most_common = self.word_counts.most_common(self.max_vocab_size - 4)\n",
        "        for idx, (word, _) in enumerate(most_common, start=4):\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "\n",
        "    def tokenize(self, text: str) -> list:\n",
        "        # Simple tokenization by lowercasing and splitting by space\n",
        "        return text.lower().split()\n",
        "\n",
        "    def encode(self, text: str, max_len: int = 256) -> list:\n",
        "        # Converts a text into a sequence of numerical indices\n",
        "        tokens = self.tokenize(text)[:max_len] # Tokenize and truncate to max_len\n",
        "        indices = [self.word2idx.get(token, 1) for token in tokens] # Map tokens to indices, using <UNK> for unknown\n",
        "        return indices\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.word2idx) # Returns the size of the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBd3fCKpWyLo"
      },
      "outputs": [],
      "source": [
        "class AGNewsDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 texts:   list,\n",
        "                 labels:  list,\n",
        "                 vocab:   Vocabulary,\n",
        "                 max_len: int = 256) -> None:\n",
        "\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts) # Returns the total number of samples\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        # Retrieves a single sample (text and label) at a given index\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        indices = self.vocab.encode(text, self.max_len) # Encode text to numerical indices\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(indices, dtype=torch.long), # Padded input token IDs\n",
        "            \"label\": torch.tensor(label, dtype=torch.long) # Label of the sample\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncdF019ZZTst"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Custom collate function to handle variable-length sequences and pad them\n",
        "    input_ids = [item[\"input_ids\"] for item in batch]\n",
        "    labels = torch.stack([item[\"label\"] for item in batch])\n",
        "\n",
        "    input_ids_padded = nn.utils.rnn.pad_sequence( # Pad sequences to the maximum length in the batch\n",
        "        input_ids,\n",
        "        batch_first=True,\n",
        "        padding_value=0 # Use 0 (PAD token index) for padding\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_padded,\n",
        "        \"label\": labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYjMM43EaBGt"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 model:        nn.Module,\n",
        "                 train_loader: DataLoader,\n",
        "                 val_loader:   DataLoader,\n",
        "                 optimizer:    torch.optim.Optimizer,\n",
        "                 criterion:    nn.Module,\n",
        "                 device:       torch.device,\n",
        "                 num_epochs:   int = 10,\n",
        "                 gradient_accumulation_steps: int = 1):\n",
        "\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        self.num_epochs = num_epochs\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.scaler = GradScaler() # For mixed precision training\n",
        "\n",
        "    async def train_epoch(self) -> Tuple[float, float]:\n",
        "        self.model.train() # Set model to training mode\n",
        "\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        self.optimizer.zero_grad() # Clear gradients at the start of epoch\n",
        "\n",
        "        for batch_idx, batch in enumerate(self.train_loader):\n",
        "            input_ids = batch[\"input_ids\"].to(self.device, non_blocking=True)\n",
        "            labels = batch[\"label\"].to(self.device, non_blocking=True)\n",
        "\n",
        "            with autocast(): # Enable mixed precision training\n",
        "                logits = self.model(input_ids)\n",
        "                loss = self.criterion(logits, labels)\n",
        "                loss = loss / self.gradient_accumulation_steps # Scale loss for gradient accumulation\n",
        "\n",
        "            self.scaler.scale(loss).backward() # Scale gradients before backward pass\n",
        "\n",
        "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0: # Perform optimization step after accumulation\n",
        "                self.scaler.unscale_(self.optimizer) # Unscale gradients\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    self.model.parameters(),\n",
        "                    max_norm=1.0\n",
        "                ) # Clip gradients to prevent exploding gradients\n",
        "                self.scaler.step(self.optimizer) # Update model parameters\n",
        "                self.scaler.update() # Update the scaler for the next iteration\n",
        "                self.optimizer.zero_grad() # Clear gradients\n",
        "\n",
        "            total_loss += loss.item() * self.gradient_accumulation_steps # Accumulate total loss\n",
        "            _, predicted = torch.max(logits, 1) # Get predicted class\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item() # Count correct predictions\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(\n",
        "                    f\"Batch {batch_idx + 1}/{len(self.train_loader)} - \"\n",
        "                    f\"Loss: {total_loss / (batch_idx + 1):.4f} - \"\n",
        "                    f\"Acc: {100 * correct / total:.2f}%\"\n",
        "                )\n",
        "\n",
        "        return total_loss / len(self.train_loader), 100 * correct / total # Return average loss and accuracy\n",
        "\n",
        "    async def validate(self) -> Tuple[float, float]:\n",
        "        self.model.eval() # Set model to evaluation mode\n",
        "\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations\n",
        "            for batch in self.val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(self.device, non_blocking=True)\n",
        "                labels = batch[\"label\"].to(self.device, non_blocking=True)\n",
        "\n",
        "                with autocast(): # Enable mixed precision\n",
        "                    logits = self.model(input_ids)\n",
        "                    loss = self.criterion(logits, labels)\n",
        "\n",
        "                total_loss += loss.item() # Accumulate loss\n",
        "                _, predicted = torch.max(logits, 1) # Get predicted class\n",
        "\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item() # Count correct predictions\n",
        "\n",
        "        return total_loss / len(self.val_loader), 100 * correct / total # Return average loss and accuracy\n",
        "\n",
        "    async def train(self):\n",
        "        best_val_acc = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"\\n{'=' * 70}\")\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
        "            print(f\"\\n{'=' * 70}\")\n",
        "\n",
        "            train_loss, train_acc = await self.train_epoch() # Train for one epoch\n",
        "            print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "            val_loss, val_acc = await self.validate() # Validate after each epoch\n",
        "            print(f\"\\nVal Loss: {val_loss:.4f} | Vall Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            if val_acc > best_val_acc: # Save best model based on validation accuracy\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(self.model.state_dict(), \"best_transformer_model.pt\") # Save model weights\n",
        "                print(\"Saved model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X98TL78kCJe4",
        "outputId": "18046307-bd0a-4c2a-8716-a53bb4102781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Set device to GPU if available, else CPU\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "MAX_VOCAB_SIZE = 30000 # Maximum size of the vocabulary\n",
        "MAX_LEN = 256 # Maximum sequence length for input texts\n",
        "\n",
        "BATCH_SIZE = 64 # Number of samples per batch\n",
        "LEARNING_RATE = 1e-4 # Learning rate for the optimizer\n",
        "NUM_EPOCHS = 10 # Number of training epochs\n",
        "\n",
        "D_MODEL = 512 # Dimension of model embeddings and attention outputs\n",
        "NUM_HEADS = 8 # Number of attention heads in MultiHeadAttention\n",
        "NUM_LAYERS = 6 # Number of encoder layers in the Transformer\n",
        "D_FF = 2048 # Dimension of the hidden layer in the PositionwiseFeedForward\n",
        "DROPOUT = 0.1 # Dropout rate\n",
        "\n",
        "NUM_CLASSES = 4 # Number of output classes (categories for AG News)\n",
        "GRADIENT_ACCUMULATION_STEPS = 2 # Number of batches to accumulate gradients over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DNAWEbNCwuy",
        "outputId": "91585a76-ac26-4172-900e-e486540e958e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vocab step...\n",
            "\n",
            "Dataset step...\n",
            "\n",
            "Data loader step...\n",
            "\n",
            "Model step...\n",
            "\n",
            "Optimizer & Loss step...\n",
            "\n",
            "Training step..\n",
            "\n",
            "Training initialiing...\n",
            "\n",
            "======================================================================\n",
            "Epoch 1/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 1.5179 - Acc: 25.58%\n",
            "Batch 200/1875 - Loss: 1.3569 - Acc: 34.22%\n",
            "Batch 300/1875 - Loss: 1.1308 - Acc: 47.08%\n",
            "Batch 400/1875 - Loss: 0.9525 - Acc: 56.83%\n",
            "Batch 500/1875 - Loss: 0.8406 - Acc: 62.83%\n",
            "Batch 600/1875 - Loss: 0.7584 - Acc: 67.18%\n",
            "Batch 700/1875 - Loss: 0.6948 - Acc: 70.36%\n",
            "Batch 800/1875 - Loss: 0.6473 - Acc: 72.76%\n",
            "Batch 900/1875 - Loss: 0.6101 - Acc: 74.65%\n",
            "Batch 1000/1875 - Loss: 0.5788 - Acc: 76.19%\n",
            "Batch 1100/1875 - Loss: 0.5538 - Acc: 77.44%\n",
            "Batch 1200/1875 - Loss: 0.5313 - Acc: 78.52%\n",
            "Batch 1300/1875 - Loss: 0.5127 - Acc: 79.45%\n",
            "Batch 1400/1875 - Loss: 0.4949 - Acc: 80.33%\n",
            "Batch 1500/1875 - Loss: 0.4795 - Acc: 81.05%\n",
            "Batch 1600/1875 - Loss: 0.4661 - Acc: 81.69%\n",
            "Batch 1700/1875 - Loss: 0.4548 - Acc: 82.24%\n",
            "Batch 1800/1875 - Loss: 0.4439 - Acc: 82.76%\n",
            "\n",
            "Train Loss: 0.4361 | Train Acc: 83.13%\n",
            "\n",
            "Val Loss: 0.2776 | Vall Acc: 91.68%\n",
            "Saved model.\n",
            "\n",
            "======================================================================\n",
            "Epoch 2/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.1907 - Acc: 94.00%\n",
            "Batch 200/1875 - Loss: 0.1938 - Acc: 93.85%\n",
            "Batch 300/1875 - Loss: 0.1900 - Acc: 93.97%\n",
            "Batch 400/1875 - Loss: 0.1907 - Acc: 93.89%\n",
            "Batch 500/1875 - Loss: 0.1912 - Acc: 93.86%\n",
            "Batch 600/1875 - Loss: 0.1931 - Acc: 93.76%\n",
            "Batch 700/1875 - Loss: 0.1946 - Acc: 93.71%\n",
            "Batch 800/1875 - Loss: 0.1958 - Acc: 93.64%\n",
            "Batch 900/1875 - Loss: 0.1948 - Acc: 93.65%\n",
            "Batch 1000/1875 - Loss: 0.1952 - Acc: 93.68%\n",
            "Batch 1100/1875 - Loss: 0.1949 - Acc: 93.67%\n",
            "Batch 1200/1875 - Loss: 0.1941 - Acc: 93.70%\n",
            "Batch 1300/1875 - Loss: 0.1939 - Acc: 93.69%\n",
            "Batch 1400/1875 - Loss: 0.1945 - Acc: 93.65%\n",
            "Batch 1500/1875 - Loss: 0.1938 - Acc: 93.68%\n",
            "Batch 1600/1875 - Loss: 0.1940 - Acc: 93.66%\n",
            "Batch 1700/1875 - Loss: 0.1934 - Acc: 93.70%\n",
            "Batch 1800/1875 - Loss: 0.1933 - Acc: 93.68%\n",
            "\n",
            "Train Loss: 0.1926 | Train Acc: 93.69%\n",
            "\n",
            "Val Loss: 0.2533 | Vall Acc: 92.13%\n",
            "Saved model.\n",
            "\n",
            "======================================================================\n",
            "Epoch 3/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.1269 - Acc: 95.80%\n",
            "Batch 200/1875 - Loss: 0.1224 - Acc: 95.91%\n",
            "Batch 300/1875 - Loss: 0.1183 - Acc: 96.07%\n",
            "Batch 400/1875 - Loss: 0.1217 - Acc: 95.98%\n",
            "Batch 500/1875 - Loss: 0.1219 - Acc: 96.01%\n",
            "Batch 600/1875 - Loss: 0.1226 - Acc: 95.99%\n",
            "Batch 700/1875 - Loss: 0.1253 - Acc: 95.94%\n",
            "Batch 800/1875 - Loss: 0.1276 - Acc: 95.86%\n",
            "Batch 900/1875 - Loss: 0.1277 - Acc: 95.84%\n",
            "Batch 1000/1875 - Loss: 0.1272 - Acc: 95.84%\n",
            "Batch 1100/1875 - Loss: 0.1272 - Acc: 95.84%\n",
            "Batch 1200/1875 - Loss: 0.1266 - Acc: 95.86%\n",
            "Batch 1300/1875 - Loss: 0.1275 - Acc: 95.82%\n",
            "Batch 1400/1875 - Loss: 0.1274 - Acc: 95.82%\n",
            "Batch 1500/1875 - Loss: 0.1282 - Acc: 95.79%\n",
            "Batch 1600/1875 - Loss: 0.1289 - Acc: 95.79%\n",
            "Batch 1700/1875 - Loss: 0.1299 - Acc: 95.75%\n",
            "Batch 1800/1875 - Loss: 0.1302 - Acc: 95.73%\n",
            "\n",
            "Train Loss: 0.1308 | Train Acc: 95.71%\n",
            "\n",
            "Val Loss: 0.2723 | Vall Acc: 91.54%\n",
            "\n",
            "======================================================================\n",
            "Epoch 4/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.0853 - Acc: 97.11%\n",
            "Batch 200/1875 - Loss: 0.0812 - Acc: 97.29%\n",
            "Batch 300/1875 - Loss: 0.0835 - Acc: 97.35%\n",
            "Batch 400/1875 - Loss: 0.0842 - Acc: 97.27%\n",
            "Batch 500/1875 - Loss: 0.0841 - Acc: 97.28%\n",
            "Batch 600/1875 - Loss: 0.0855 - Acc: 97.26%\n",
            "Batch 700/1875 - Loss: 0.0858 - Acc: 97.26%\n",
            "Batch 800/1875 - Loss: 0.0853 - Acc: 97.25%\n",
            "Batch 900/1875 - Loss: 0.0842 - Acc: 97.29%\n",
            "Batch 1000/1875 - Loss: 0.0852 - Acc: 97.25%\n",
            "Batch 1100/1875 - Loss: 0.0854 - Acc: 97.25%\n",
            "Batch 1200/1875 - Loss: 0.0863 - Acc: 97.21%\n",
            "Batch 1300/1875 - Loss: 0.0863 - Acc: 97.19%\n",
            "Batch 1400/1875 - Loss: 0.0873 - Acc: 97.16%\n",
            "Batch 1500/1875 - Loss: 0.0879 - Acc: 97.14%\n",
            "Batch 1600/1875 - Loss: 0.0885 - Acc: 97.10%\n",
            "Batch 1700/1875 - Loss: 0.0886 - Acc: 97.11%\n",
            "Batch 1800/1875 - Loss: 0.0890 - Acc: 97.09%\n",
            "\n",
            "Train Loss: 0.0901 | Train Acc: 97.04%\n",
            "\n",
            "Val Loss: 0.3586 | Vall Acc: 91.33%\n",
            "\n",
            "======================================================================\n",
            "Epoch 5/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.0510 - Acc: 98.56%\n",
            "Batch 200/1875 - Loss: 0.0553 - Acc: 98.32%\n",
            "Batch 300/1875 - Loss: 0.0545 - Acc: 98.34%\n",
            "Batch 400/1875 - Loss: 0.0555 - Acc: 98.28%\n",
            "Batch 500/1875 - Loss: 0.0561 - Acc: 98.26%\n",
            "Batch 600/1875 - Loss: 0.0560 - Acc: 98.25%\n",
            "Batch 700/1875 - Loss: 0.0570 - Acc: 98.22%\n",
            "Batch 800/1875 - Loss: 0.0580 - Acc: 98.18%\n",
            "Batch 900/1875 - Loss: 0.0582 - Acc: 98.16%\n",
            "Batch 1000/1875 - Loss: 0.0577 - Acc: 98.20%\n",
            "Batch 1100/1875 - Loss: 0.0587 - Acc: 98.17%\n",
            "Batch 1200/1875 - Loss: 0.0587 - Acc: 98.17%\n",
            "Batch 1300/1875 - Loss: 0.0596 - Acc: 98.14%\n",
            "Batch 1400/1875 - Loss: 0.0600 - Acc: 98.12%\n",
            "Batch 1500/1875 - Loss: 0.0598 - Acc: 98.12%\n",
            "Batch 1600/1875 - Loss: 0.0610 - Acc: 98.08%\n",
            "Batch 1700/1875 - Loss: 0.0616 - Acc: 98.04%\n",
            "Batch 1800/1875 - Loss: 0.0621 - Acc: 98.03%\n",
            "\n",
            "Train Loss: 0.0625 | Train Acc: 98.02%\n",
            "\n",
            "Val Loss: 0.3964 | Vall Acc: 91.20%\n",
            "\n",
            "======================================================================\n",
            "Epoch 6/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.0321 - Acc: 99.02%\n",
            "Batch 200/1875 - Loss: 0.0418 - Acc: 98.69%\n",
            "Batch 300/1875 - Loss: 0.0437 - Acc: 98.67%\n",
            "Batch 400/1875 - Loss: 0.0424 - Acc: 98.72%\n",
            "Batch 500/1875 - Loss: 0.0436 - Acc: 98.69%\n",
            "Batch 600/1875 - Loss: 0.0425 - Acc: 98.73%\n",
            "Batch 700/1875 - Loss: 0.0421 - Acc: 98.75%\n",
            "Batch 800/1875 - Loss: 0.0416 - Acc: 98.74%\n",
            "Batch 900/1875 - Loss: 0.0420 - Acc: 98.70%\n",
            "Batch 1000/1875 - Loss: 0.0427 - Acc: 98.68%\n",
            "Batch 1100/1875 - Loss: 0.0433 - Acc: 98.66%\n",
            "Batch 1200/1875 - Loss: 0.0434 - Acc: 98.64%\n",
            "Batch 1300/1875 - Loss: 0.0439 - Acc: 98.62%\n",
            "Batch 1400/1875 - Loss: 0.0436 - Acc: 98.62%\n",
            "Batch 1500/1875 - Loss: 0.0435 - Acc: 98.62%\n",
            "Batch 1600/1875 - Loss: 0.0437 - Acc: 98.62%\n",
            "Batch 1700/1875 - Loss: 0.0448 - Acc: 98.59%\n",
            "Batch 1800/1875 - Loss: 0.0452 - Acc: 98.58%\n",
            "\n",
            "Train Loss: 0.0456 | Train Acc: 98.57%\n",
            "\n",
            "Val Loss: 0.4658 | Vall Acc: 90.83%\n",
            "\n",
            "======================================================================\n",
            "Epoch 7/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.0249 - Acc: 99.22%\n",
            "Batch 200/1875 - Loss: 0.0259 - Acc: 99.22%\n",
            "Batch 300/1875 - Loss: 0.0263 - Acc: 99.21%\n",
            "Batch 400/1875 - Loss: 0.0260 - Acc: 99.23%\n",
            "Batch 500/1875 - Loss: 0.0258 - Acc: 99.21%\n",
            "Batch 600/1875 - Loss: 0.0271 - Acc: 99.16%\n",
            "Batch 700/1875 - Loss: 0.0294 - Acc: 99.08%\n",
            "Batch 800/1875 - Loss: 0.0301 - Acc: 99.08%\n",
            "Batch 900/1875 - Loss: 0.0307 - Acc: 99.07%\n",
            "Batch 1000/1875 - Loss: 0.0313 - Acc: 99.05%\n",
            "Batch 1100/1875 - Loss: 0.0310 - Acc: 99.07%\n",
            "Batch 1200/1875 - Loss: 0.0311 - Acc: 99.07%\n",
            "Batch 1300/1875 - Loss: 0.0316 - Acc: 99.06%\n",
            "Batch 1400/1875 - Loss: 0.0316 - Acc: 99.05%\n",
            "Batch 1500/1875 - Loss: 0.0320 - Acc: 99.02%\n",
            "Batch 1600/1875 - Loss: 0.0326 - Acc: 99.01%\n",
            "Batch 1700/1875 - Loss: 0.0331 - Acc: 98.97%\n",
            "Batch 1800/1875 - Loss: 0.0337 - Acc: 98.96%\n",
            "\n",
            "Train Loss: 0.0338 | Train Acc: 98.96%\n",
            "\n",
            "Val Loss: 0.4691 | Vall Acc: 91.30%\n",
            "\n",
            "======================================================================\n",
            "Epoch 8/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.0170 - Acc: 99.59%\n",
            "Batch 200/1875 - Loss: 0.0194 - Acc: 99.51%\n",
            "Batch 300/1875 - Loss: 0.0226 - Acc: 99.39%\n",
            "Batch 400/1875 - Loss: 0.0229 - Acc: 99.37%\n",
            "Batch 500/1875 - Loss: 0.0245 - Acc: 99.33%\n",
            "Batch 600/1875 - Loss: 0.0255 - Acc: 99.31%\n",
            "Batch 700/1875 - Loss: 0.0244 - Acc: 99.33%\n",
            "Batch 800/1875 - Loss: 0.0247 - Acc: 99.30%\n",
            "Batch 900/1875 - Loss: 0.0241 - Acc: 99.31%\n",
            "Batch 1000/1875 - Loss: 0.0238 - Acc: 99.31%\n",
            "Batch 1100/1875 - Loss: 0.0242 - Acc: 99.30%\n",
            "Batch 1200/1875 - Loss: 0.0252 - Acc: 99.25%\n",
            "Batch 1300/1875 - Loss: 0.0256 - Acc: 99.23%\n",
            "Batch 1400/1875 - Loss: 0.0261 - Acc: 99.20%\n",
            "Batch 1500/1875 - Loss: 0.0265 - Acc: 99.18%\n",
            "Batch 1600/1875 - Loss: 0.0268 - Acc: 99.18%\n",
            "Batch 1700/1875 - Loss: 0.0273 - Acc: 99.16%\n",
            "Batch 1800/1875 - Loss: 0.0275 - Acc: 99.15%\n",
            "\n",
            "Train Loss: 0.0278 | Train Acc: 99.15%\n",
            "\n",
            "Val Loss: 0.4851 | Vall Acc: 91.16%\n",
            "\n",
            "======================================================================\n",
            "Epoch 9/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.0146 - Acc: 99.59%\n",
            "Batch 200/1875 - Loss: 0.0156 - Acc: 99.56%\n",
            "Batch 300/1875 - Loss: 0.0163 - Acc: 99.53%\n",
            "Batch 400/1875 - Loss: 0.0170 - Acc: 99.50%\n",
            "Batch 500/1875 - Loss: 0.0161 - Acc: 99.51%\n",
            "Batch 600/1875 - Loss: 0.0177 - Acc: 99.48%\n",
            "Batch 700/1875 - Loss: 0.0203 - Acc: 99.42%\n",
            "Batch 800/1875 - Loss: 0.0211 - Acc: 99.39%\n",
            "Batch 900/1875 - Loss: 0.0207 - Acc: 99.40%\n",
            "Batch 1000/1875 - Loss: 0.0206 - Acc: 99.40%\n",
            "Batch 1100/1875 - Loss: 0.0223 - Acc: 99.36%\n",
            "Batch 1200/1875 - Loss: 0.0224 - Acc: 99.36%\n",
            "Batch 1300/1875 - Loss: 0.0227 - Acc: 99.35%\n",
            "Batch 1400/1875 - Loss: 0.0232 - Acc: 99.32%\n",
            "Batch 1500/1875 - Loss: 0.0237 - Acc: 99.31%\n",
            "Batch 1600/1875 - Loss: 0.0243 - Acc: 99.30%\n",
            "Batch 1700/1875 - Loss: 0.0246 - Acc: 99.29%\n",
            "Batch 1800/1875 - Loss: 0.0248 - Acc: 99.28%\n",
            "\n",
            "Train Loss: 0.0247 | Train Acc: 99.28%\n",
            "\n",
            "Val Loss: 0.5205 | Vall Acc: 91.05%\n",
            "\n",
            "======================================================================\n",
            "Epoch 10/10\n",
            "\n",
            "======================================================================\n",
            "Batch 100/1875 - Loss: 0.0198 - Acc: 99.44%\n",
            "Batch 200/1875 - Loss: 0.0176 - Acc: 99.50%\n",
            "Batch 300/1875 - Loss: 0.0181 - Acc: 99.49%\n",
            "Batch 400/1875 - Loss: 0.0198 - Acc: 99.45%\n",
            "Batch 500/1875 - Loss: 0.0186 - Acc: 99.48%\n",
            "Batch 600/1875 - Loss: 0.0189 - Acc: 99.47%\n",
            "Batch 700/1875 - Loss: 0.0179 - Acc: 99.47%\n",
            "Batch 800/1875 - Loss: 0.0177 - Acc: 99.47%\n",
            "Batch 900/1875 - Loss: 0.0173 - Acc: 99.48%\n",
            "Batch 1000/1875 - Loss: 0.0174 - Acc: 99.46%\n",
            "Batch 1100/1875 - Loss: 0.0175 - Acc: 99.45%\n",
            "Batch 1200/1875 - Loss: 0.0182 - Acc: 99.43%\n",
            "Batch 1300/1875 - Loss: 0.0185 - Acc: 99.42%\n",
            "Batch 1400/1875 - Loss: 0.0184 - Acc: 99.42%\n",
            "Batch 1500/1875 - Loss: 0.0188 - Acc: 99.41%\n",
            "Batch 1600/1875 - Loss: 0.0191 - Acc: 99.40%\n",
            "Batch 1700/1875 - Loss: 0.0192 - Acc: 99.40%\n",
            "Batch 1800/1875 - Loss: 0.0195 - Acc: 99.39%\n",
            "\n",
            "Train Loss: 0.0196 | Train Acc: 99.39%\n",
            "\n",
            "Val Loss: 0.6578 | Vall Acc: 90.95%\n",
            "\n",
            "Training finished!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nVocab step...\")\n",
        "vocab = Vocabulary(max_vocab_size=MAX_VOCAB_SIZE) # Initialize vocabulary\n",
        "vocab.build_vocab(train_texts) # Build vocabulary from training texts\n",
        "\n",
        "print(\"\\nDataset step...\")\n",
        "train_dataset = AGNewsDataset(train_texts, train_labels, vocab, MAX_LEN) # Create training dataset\n",
        "val_dataset = AGNewsDataset(test_texts, test_labels, vocab, MAX_LEN) # Create validation dataset\n",
        "\n",
        "print(\"\\nData loader step...\")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn # Use custom collate function for padding\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn # Use custom collate function for padding\n",
        ")\n",
        "\n",
        "print(\"\\nModel step...\")\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    num_classes=NUM_CLASSES,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    d_ff=D_FF,\n",
        "    max_len=MAX_LEN,\n",
        "    dropout=DROPOUT,\n",
        "    pad_idx=0\n",
        ").to(device) # Initialize and move model to specified device\n",
        "\n",
        "print(\"\\nOptimizer & Loss step...\")\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=0.01\n",
        ") # Initialize AdamW optimizer\n",
        "criterion = nn.CrossEntropyLoss() # Initialize Cross-Entropy loss function\n",
        "\n",
        "print(\"\\nTraining step..\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n",
        ") # Initialize the Trainer\n",
        "\n",
        "print(\"\\nTraining initialiing...\")\n",
        "await trainer.train() # Start the training process\n",
        "\n",
        "print(\"\\nTraining finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj3xyDQ4C_Az",
        "outputId": "11f95f19-cf28-4663-e48d-3761b683119a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'Tesla announces new electric vehicle with 500 mile range',\n",
              " 'predicted_class': 3,\n",
              " 'predicted_label': 'Sci/Tech',\n",
              " 'probabilities': array([2.5714253e-05, 3.4453541e-07, 4.1499137e-05, 9.9993241e-01],\n",
              "       dtype=float32),\n",
              " 'confidence': 0.9999324083328247}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "text = \"Tesla announces new electric vehicle with 500 mile range\" # Sample text for prediction\n",
        "\n",
        "indices = vocab.encode(text, MAX_LEN) # Encode the text into numerical indices\n",
        "input_ids = torch.tensor(indices, dtype=torch.long).unsqueeze(0) # Convert to tensor and add batch dimension\n",
        "input_ids = input_ids.to(device) # Move input to the correct device\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    logits = model(input_ids) # Get raw output from the model\n",
        "    probabilities = F.softmax(logits, dim=-1) # Convert logits to probabilities\n",
        "    predicted_class = torch.argmax(probabilities, dim=-1).item() # Get the class with the highest probability\n",
        "\n",
        "output_result = {\n",
        "    'text': text,\n",
        "    'predicted_class': predicted_class,\n",
        "    'predicted_label': label_map[predicted_class], # Map predicted class ID to label name\n",
        "    'probabilities': probabilities.cpu().numpy()[0],\n",
        "    'confidence': probabilities[0, predicted_class].item()\n",
        "}\n",
        "\n",
        "output_result # Display the prediction results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "nepCc-vZKUkD",
        "outputId": "429fe923-0efd-4340-a65d-f44c7ab3f35e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "all_preds = [] # List to store all predictions\n",
        "all_labels = [] # List to store all true labels\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculations\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
        "        labels = batch[\"label\"].to(self.device, non_blocking=True)\n",
        "\n",
        "        logits = model(input_ids) # Get model outputs\n",
        "        _, predicted = torch.max(logits, 1) # Get predicted class for each sample in batch\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy()) # Store predictions\n",
        "        all_labels.extend(labels.cpu().numpy()) # Store true labels\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds) # Calculate the confusion matrix\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True, # Annotate cells with data values\n",
        "    fmt=\"d\", # Format annotations as integers\n",
        "    cmap=\"Blues\", # Color map for the heatmap\n",
        "    xticklabels=[label_map[i] for i in sorted(label_map.keys())], # X-axis labels (predicted)\n",
        "    yticklabels=[label_map[i] for i in sorted(label_map.keys())] # Y-axis labels (true)\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Predicted Label\") # X-axis title\n",
        "plt.ylabel(\"True Label\") # Y-axis title\n",
        "plt.title(\"Confusion Matrix\") # Plot title\n",
        "plt.show() # Display the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys9qH7TwR2TL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "transformers-standard-model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
